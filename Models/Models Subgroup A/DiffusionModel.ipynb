{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diffusion model based on Denoising Diffusion Probabilistic Model (DDPM)\n",
    "\n",
    "Generative model that uses a process of progressively adding noise to an image, then learning to reverse this process to generate new, high-quality images.\n",
    "\n",
    "unet (UNet2DModel) — Deep Learning Architecture consisting of encoders and decoders.\n",
    "\n",
    "scheduler (SchedulerMixin) — Controls amount of noise added to image. To be used in combination with unet to denoise the encoded image. Can be one of DDPMScheduler, or DDIMScheduler.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install diffusers transformers torch datasets torchvision Pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the following libraries:\n",
    "\n",
    "pip install diffusers transformers torch datasets torchvision Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import UNet2DModel, DDPMScheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Resize and normalise input images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load Images\n",
    "class CustomImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith(\".webp\")] #change to filetype of dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "dataset = CustomImageDataset(\"C:/Users/BurnD/Desktop/DSA3101/Dataset/Red2\", transform=transform) #change the path to where the images are stored\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=True) \n",
    "\n",
    "# Initialize the UNet model\n",
    "model = UNet2DModel(\n",
    "    sample_size=256,         # Image resolution\n",
    "    in_channels=3,          # RGB images\n",
    "    out_channels=3,         # Predicting RGB noise\n",
    "    layers_per_block=2,\n",
    "    block_out_channels=(64, 128, 256, 512),  # Number of channels for each layer\n",
    "    down_block_types=(\"DownBlock2D\", \"DownBlock2D\", \"DownBlock2D\", \"DownBlock2D\"),\n",
    "    up_block_types=(\"UpBlock2D\", \"UpBlock2D\", \"UpBlock2D\", \"UpBlock2D\"),\n",
    ")\n",
    "\n",
    "# Define the noise scheduler\n",
    "scheduler = DDPMScheduler(num_train_timesteps=500, beta_start=0.00005, beta_end=0.01, beta_schedule = 'scaled_linear')\n",
    "\n",
    "# Set up the optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataloader Parameters:\n",
    "\n",
    "dataset: Dataset containing the images to be used\n",
    "\n",
    "shuffle: Determines whether the data should be shuffled randomly at each epoch. Setting shuffle=True means that the order of samples will be randomized for each new epoch, which can help improve model generalization by reducing the chance of learning patterns that depend on the order of the data.\n",
    "\n",
    "batch_size: Specifies the number of samples in each batch. Using batches (rather than individual samples) speeds up training by allowing simultaneous processing of multiple samples.\n",
    "\n",
    "DDPM Parameters:\n",
    "\n",
    "num_train_timesteps: number of diffusion steps\n",
    "\n",
    "beta_start: starting beta value of inference (amount of noise added at the first time step)\n",
    "\n",
    "beta_end: final beta value (amount of noise added at the last timestep)\n",
    "\n",
    "beta_schedule: how the noise should scale over the timesteps \n",
    "\n",
    "Optimiser:\n",
    "\n",
    "AdamW: Variant of Adam Optimiser with weight decay for better regularisation\n",
    "\n",
    "lr: Learning Rate, in this case, learning rate = 0.0001\n",
    "\n",
    "U-Net Model:\n",
    "\n",
    "sample_size: Image Resolution\n",
    "\n",
    "in_channels: number of input channels (3 channels = RGB)\n",
    "\n",
    "out_channels: number of output channels\n",
    "\n",
    "layers_per_block: number of convolutional layers in each block\n",
    "\n",
    "block_out_channel: output channels for each block in the encoder and decoder. 64 -> 128: more features captured as spatial size decreases\n",
    "\n",
    "down_block_types: blocks used in encoder (reduce spatial dimension, learning patterns and features)\n",
    "\n",
    "up_block_types: blocks used in decoder (reconstruction from compressed image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10  # Define number of epochs\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for step, images in enumerate(tqdm(dataloader)):\n",
    "        images = images.to(device)\n",
    "\n",
    "        # Sample random noise and timesteps for each image\n",
    "        noise = torch.randn_like(images).to(device)\n",
    "        timesteps = torch.randint(0, scheduler.num_train_timesteps, (images.size(0),), device=device).long()\n",
    "\n",
    "        # Add noise to images based on the timesteps (i.e., create noisy images)\n",
    "        noisy_images = scheduler.add_noise(images, noise, timesteps)\n",
    "\n",
    "        # Forward pass: Predict the noise\n",
    "        noise_pred = model(noisy_images, timesteps).sample\n",
    "\n",
    "        # Calculate the loss (mean squared error between predicted noise and actual noise)\n",
    "        loss = torch.nn.functional.mse_loss(noise_pred, noise)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % 10 == 0:\n",
    "            print(f\"Epoch [{epoch + 1}/{epochs}], Step [{step}/{len(dataloader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "epochs: number of times the dataset is passed through the model\n",
    "\n",
    "tqdm: visual display of progress\n",
    "\n",
    "optimizer.zero_grad(): clears any existing gradients\n",
    "\n",
    "loss.backward(): computes gradient of loss wrt model parameters\n",
    "\n",
    "optimizer.step(): updates parameters based on calculated gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"diffusion_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDPMPipeline\n",
    "import torch\n",
    "\n",
    "# Load the model and scheduler\n",
    "model = UNet2DModel.from_pretrained(\"diffusion_model\")\n",
    "scheduler = DDPMScheduler(num_train_timesteps=200)\n",
    "\n",
    "# Generate images by reversing the noise process\n",
    "model.to(device).eval()\n",
    "\n",
    "num_samples = 4\n",
    "with torch.no_grad():\n",
    "    for i in range(num_samples):\n",
    "        # Start with random noise\n",
    "        image = torch.randn(1, 3, 64, 64).to(device)\n",
    "\n",
    "        # Perform reverse diffusion to denoise step-by-step\n",
    "        for t in reversed(range(scheduler.num_train_timesteps)):\n",
    "            # Predict noise and update image\n",
    "            noise_pred = model(image, torch.tensor([t]).to(device)).sample\n",
    "            image = scheduler.step(noise_pred, t, image).prev_sample\n",
    "\n",
    "        # Save or display the generated image\n",
    "        image = (image.clamp(-1, 1) + 1) / 2  # Rescale to [0, 1]\n",
    "        image = transforms.ToPILImage()(image.squeeze().cpu()) #converts the tensor to a PIL format for saving\n",
    "        image.save(f\"generated_image_{i}.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.randn(batch, channel, resolution, resolution)\n",
    "torch.randn(1, 3, 64, 64): 1 batch with 3 colour channels of resolution 64 x 64\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
